import sklearn.datasets
import nltk.stem
MLCOMP_DIR = r"./data"
data = sklearn.datasets.load_mlcomp("20news-18828", mlcomp_root=MLCOMP_DIR)

print(data.filenames)
print(len(data.filenames))
print(data.target_names)

#train_data = sklearn.datasets.load_mlcomp("20news-18828", "train",mlcomp_root=MLCOMP_DIR)
#print(len(train_data.filenames))
#
#test_data = sklearn.datasets.load_mlcomp("20news-18828","test", mlcomp_root=MLCOMP_DIR)
#print(len(test_data.filenames))

groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.ma c.hardware', 'comp.windows.x', 'sci.space']
train_data = sklearn.datasets.load_mlcomp("20news-18828", "train",mlcomp_root=MLCOMP_DIR, categories=groups)
print(len(train_data.filenames))

english_stemmer = nltk.stem.SnowballStemmer('english')
#TF-IDF
class StemmedTfidfVectorizer(TfidfVectorizer):
    def build_analyzer(self):
        analyzer = super(TfidfVectorizer,self).build_analyzer()
        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))

vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,stop_words='english', charset_error='ignore')
vectorized = vectorizer.fit_transform(dataset.data)

num_samples, num_features = vectorized.shape
print("#samples: %d, #features: %d" % (num_samples, num_features))